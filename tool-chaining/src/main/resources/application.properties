server.port=8080
# ==============================================================================
# Spring AI Ollama Configuration
# ==============================================================================

# 1. Enable Ollama as the primary chat model provider
spring.ai.model.chat=ollama

# 2. Configure the Llama model name
# IMPORTANT: This model MUST be pulled and running in your local Ollama instance (e.g., 'ollama pull llama3.1')
spring.ai.ollama.chat.options.model=llama3.1

# 3. Configure the base URL for Ollama (default is usually correct)
# spring.ai.ollama.base-url=http://localhost:11434

# 4. Set a low temperature for reliable tool/function calling
# A low temperature (0.0 to 0.1) encourages the model to be deterministic and trigger tools correctly.
spring.ai.ollama.chat.options.temperature=0.0

# 5. Disable model pulling on startup (optional, but good practice)
spring.ai.ollama.init.pull-model-strategy=never

api.user.base-url=http://localhost:8085
api.order.base-url=http://localhost:8090
api.transaction.base-url=http://localhost:8095
